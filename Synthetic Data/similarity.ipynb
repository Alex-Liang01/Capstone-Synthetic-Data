{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from sdv.metadata import Metadata\n",
    "\n",
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "from sdv.evaluation.single_table import get_column_pair_plot\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sdmetrics.column_pairs.statistical.contingency_similarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in simulated data\n",
    "def readSimulatedData(folder,dependence,path):\n",
    "    path = f\"{path}/{folder}/\"\n",
    "    dtypes = {'countryCode': 'category', 'Language': 'category', 'gender': 'category', 'pilStatus': 'category'}\n",
    "    if folder==\"Simulated Data\":\n",
    "        for row in rows:\n",
    "            for missing in missings:\n",
    "                file_name = f\"{row}_obs_{missing}_percent_missing.csv\"\n",
    "                name=f\"data_{row}_{missing}\"\n",
    "                globals()[name]= pd.read_csv(path + file_name, dtype=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in synthetic data\n",
    "def readSyntheticData(dependence,folder_path):\n",
    "    synthetic_types=['GC','CTGAN','TVAE']\n",
    "\n",
    "    path = f\"{folder_path}/{dependence}/Synthetic Datasets/\"\n",
    "    dtypes = {'countryCode': 'category', 'Language': 'category', 'gender': 'category', 'pilStatus': 'category'}\n",
    "    if dependence==\"Synthetic Data\":\n",
    "        for row in rows:\n",
    "            for missing in missings:\n",
    "                    for synthetic_type in synthetic_types:\n",
    "                        file_synthetic_type=str.lower(synthetic_type) \n",
    "                        file_name = f\"{row} rows_synthetic_{file_synthetic_type}.csv\"\n",
    "                        name=f\"{synthetic_type}_data_{row}_{missing}\"\n",
    "        \n",
    "                        folder_name=f\"{row} row {missing} missing/{synthetic_type}/\"\n",
    "                        globals()[name]= pd.read_csv(path + folder_name + file_name,dtype=dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lists for 0%, 10%, 20% missing data (Simulated Non synthetic data)\n",
    "def simulated_lists(dependences,rows,missings):\n",
    "    for dependence in dependences:\n",
    "        for row in rows:\n",
    "            name=f\"{dependence}_{row}\"\n",
    "            globals()[name]=[]\n",
    "            for missing in missings:\n",
    "                    if dependence==\"data\":\n",
    "                        variable=f'data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])\n",
    "                    else:\n",
    "                        variable=f'{dependence}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of synthetic data sets. Prints the name of the lists. 0 = 0%, 10 = 10%, 20 = 20%\n",
    "def synthesizer_lists(synthesizers,dependences,rows,missings):\n",
    "    for synthesizer in synthesizers:\n",
    "        for dependence in dependences:\n",
    "            if dependence==\"data\":\n",
    "                for row in rows:\n",
    "                    name=f\"{synthesizer}_{row}_{dependence}\"\n",
    "                    globals()[name]=[]\n",
    "                    \n",
    "                    for missing in missings:\n",
    "                        variable=f'{synthesizer}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])\n",
    "            else:\n",
    "                for row in rows:\n",
    "                    name=f\"{synthesizer}_{row}_{dependence}\"\n",
    "                    globals()[name]=[]\n",
    "                    for missing in missings:\n",
    "                        variable=f'{synthesizer}_{dependence}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the lists of data frames for each synthesizer/simulated data type. All synthesizer dataframes in one list\n",
    "def mergeList(list1,list2,list3):\n",
    "    merged_list = list1+list2+list3\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating metadata for synthesizers\n",
    "def createMetadata(df_list):\n",
    "    metadata_list=[]\n",
    "    for df in df_list:\n",
    "        metadata = Metadata.detect_from_dataframe(data=df)\n",
    "        metadata.update_column(column_name=\"arcsId\", sdtype=\"id\")\n",
    "        metadata.update_column(column_name=\"hashedId\", sdtype=\"id\")\n",
    "        metadata.update_column(column_name=\"countryCode\",sdtype=\"categorical\")\n",
    "        metadata.update_column(column_name=\"Language\",sdtype=\"categorical\")\n",
    "        metadata_list.append(metadata)\n",
    "        \n",
    "    return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age feature from date of birth\n",
    "def age(df):\n",
    "    today=datetime.today()\n",
    "    df['dateOfBirth']=pd.to_datetime(df['dateOfBirth'])\n",
    "    df['age']=df['dateOfBirth'].apply(lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing datatypes\n",
    "def preprocessing(df_list):\n",
    "    categorical_cols=['countryCode','gender','pilStatus','Language']\n",
    "    for df in df_list:\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        df[categorical_cols]=df[categorical_cols].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating reports for KSComplement and TVComplement\n",
    "def make_report(sim,syn,meta):\n",
    "    diagnostic = run_diagnostic(\n",
    "    real_data=sim,\n",
    "    synthetic_data=syn,\n",
    "    metadata=meta\n",
    "    )\n",
    "    quality_report = evaluate_quality(\n",
    "    real_data=sim,\n",
    "    synthetic_data=syn,\n",
    "    metadata=meta\n",
    "    )\n",
    "    return (diagnostic,quality_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping for reports for all datasets\n",
    "def report_loop(sim,syn,meta):\n",
    "    report_list=[]\n",
    "    for i,df in enumerate(sim):\n",
    "        report=make_report(sim[i],syn[i],meta[i])\n",
    "        report_list.append(report)\n",
    "    return report_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting column metrics\n",
    "def get_column_metrics(report_list,Synthesizer_type,df_type):\n",
    "    df = [] \n",
    "    for i,item in enumerate(report_list):   \n",
    "        columns_scores= item[1].get_properties()\n",
    "        columns_scores['df_type']=df_type[i]\n",
    "        df.append(columns_scores)\n",
    "    df=pd.concat(df, axis=0, ignore_index=True)\n",
    "\n",
    "    df['synthesizer_type']=Synthesizer_type\n",
    "    return df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting marginal metrics\n",
    "def get_marginal_metrics(report_list,Synthesizer_type,df_type):\n",
    "    df = [] \n",
    "    for i,item in enumerate(report_list):   \n",
    "        columns_scores= item[1].get_details('Column Shapes')\n",
    "        columns_scores['df_type']=df_type[i]\n",
    "        df.append(columns_scores)\n",
    "    df=pd.concat(df, axis=0, ignore_index=True)\n",
    "\n",
    "    df['synthesizer_type']=Synthesizer_type\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths\n",
    "script_dir = pathlib.Path().resolve()\n",
    "os.chdir(script_dir)\n",
    "parent_dir=script_dir.parent\n",
    "\n",
    "# Lists for dataframe sample sizes and missing data\n",
    "df_type = ['10000 row 0 missing','10000 row 10 missing','10000 row 20 missing',\n",
    "             '25000 row 0 missing','25000 row 10 missing','25000 row 20 missing',\n",
    "             '50000 row 0 missing','50000 row 10 missing','50000 row 20 missing']\n",
    "rows = [10000, 25000, 50000]\n",
    "missings = [0, 10, 20]\n",
    "\n",
    "#Reading in data\n",
    "readSimulatedData(\"Simulated Data\",\"simulated_data\",parent_dir)\n",
    "readSyntheticData(\"Synthetic Data\",parent_dir)\n",
    "\n",
    "#Putting all simulated and synthetic datasets into lists for easier processing\n",
    "simulated_lists([\"data\"],[\"10000\",\"25000\",\"50000\"],[\"0\",\"10\",\"20\"])\n",
    "synthesizer_lists([\"GC\",\"CTGAN\",\"TVAE\"],[\"data\"],[\"10000\",\"25000\",\"50000\"],[\"0\",\"10\",\"20\"])\n",
    "Simulated_data=mergeList(data_10000, data_25000, data_50000)\n",
    "GC_data=mergeList(GC_10000_data,GC_25000_data,GC_50000_data)\n",
    "CTGAN_data=mergeList(CTGAN_10000_data,CTGAN_25000_data,CTGAN_50000_data)\n",
    "TVAE_data=mergeList(TVAE_10000_data,TVAE_25000_data,TVAE_50000_data)\n",
    "\n",
    "# Creating metadata for column metrics\n",
    "metadata=createMetadata(Simulated_data)\n",
    "\n",
    "#Preprocessing lists of dataframes\n",
    "preprocessing(Simulated_data)\n",
    "preprocessing(GC_data)\n",
    "preprocessing(CTGAN_data)\n",
    "preprocessing(TVAE_data)\n",
    "\n",
    "#Dictionary for easier processing of column metric and marginal metric reports\n",
    "all_dataset={\n",
    "    'Simulated':Simulated_data,\n",
    "    'GC':GC_data,\n",
    "    'CTGAN':CTGAN_data,\n",
    "    'TVAE':TVAE_data,\n",
    "    'Metadata':metadata,\n",
    "}\n",
    "\n",
    "# Running all column and marginal metrics for each dataset\n",
    "gc_report_list=report_loop(all_dataset['Simulated'],all_dataset['GC'],all_dataset['Metadata'])\n",
    "ctgan_report_list=report_loop(all_dataset['Simulated'],all_dataset['CTGAN'],all_dataset['Metadata'])\n",
    "tvae_report_list=report_loop(all_dataset['Simulated'],all_dataset['TVAE'],all_dataset['Metadata'])\n",
    "\n",
    "# Extracting column scores\n",
    "gc_columnsScores=get_column_metrics(gc_report_list,\"GC\",df_type)\n",
    "ctgan_columnsScores=get_column_metrics(ctgan_report_list,\"CTGAN\",df_type)\n",
    "tvae_columnsScores=get_column_metrics(tvae_report_list,\"TVAE\",df_type)\n",
    "columnScores=pd.concat([gc_columnsScores,ctgan_columnsScores,tvae_columnsScores],axis=0,ignore_index=True)\n",
    "\n",
    "# Extracting marginal scores\n",
    "gc_marginalScores=get_marginal_metrics(gc_report_list,\"GC\",df_type)\n",
    "ctgan_marginalScores=get_marginal_metrics(ctgan_report_list,\"CTGAN\",df_type)\n",
    "tvae_marginalScores=get_marginal_metrics(tvae_report_list,\"TVAE\",df_type)\n",
    "marginalScores=pd.concat([gc_marginalScores,ctgan_marginalScores,tvae_marginalScores],axis=0,ignore_index=True)\n",
    "\n",
    "#Saving data\n",
    "marginalScores.to_csv('marginalScores1.csv',index=False)\n",
    "columnScores.to_csv('columnScores1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
