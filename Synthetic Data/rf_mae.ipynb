{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd;\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in simulated data\n",
    "def readSimulatedData(folder,dependence,path):\n",
    "    path = f\"{path}/{folder}/\"\n",
    "    dtypes = {'countryCode': 'category', 'Language': 'category', 'gender': 'category', 'pilStatus': 'category'}\n",
    "    if folder==\"Simulated Data\":\n",
    "        for row in rows:\n",
    "            for missing in missings:\n",
    "                file_name = f\"{row}_obs_{missing}_percent_missing.csv\"\n",
    "                name=f\"data_{row}_{missing}\"\n",
    "                globals()[name]= pd.read_csv(path + file_name, dtype=dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in synthetic data\n",
    "def readSyntheticData(dependence,folder_path):\n",
    "    synthetic_types=['GC','CTGAN','TVAE']\n",
    "\n",
    "    path = f\"{folder_path}/{dependence}/Synthetic Datasets/\"\n",
    "    dtypes = {'countryCode': 'category', 'Language': 'category', 'gender': 'category', 'pilStatus': 'category'}\n",
    "    if dependence==\"Synthetic Data\":\n",
    "        for row in rows:\n",
    "            for missing in missings:\n",
    "                    for synthetic_type in synthetic_types:\n",
    "                        file_synthetic_type=str.lower(synthetic_type) \n",
    "                        file_name = f\"{row} rows_synthetic_{file_synthetic_type}.csv\"\n",
    "                        name=f\"{synthetic_type}_data_{row}_{missing}\"\n",
    "                        folder_name=f\"{row} row {missing} missing/{synthetic_type}/\"\n",
    "                        globals()[name]= pd.read_csv(path + folder_name + file_name,dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating lists for 0%, 10%, 20% missing data\n",
    "def simulated_lists(dependences,rows,missings):\n",
    "    for dependence in dependences:\n",
    "        for row in rows:\n",
    "            name=f\"{dependence}_{row}\"\n",
    "            globals()[name]=[]\n",
    "            for missing in missings:\n",
    "                    if dependence==\"data\":\n",
    "                        variable=f'data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])\n",
    "                    else:\n",
    "                        variable=f'{dependence}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of synthetic data sets. \n",
    "def synthesizer_lists(synthesizers,dependences,rows,missings):\n",
    "    for synthesizer in synthesizers:\n",
    "        for dependence in dependences:\n",
    "            if dependence==\"data\":\n",
    "                for row in rows:\n",
    "                    name=f\"{synthesizer}_{row}_{dependence}\"\n",
    "                    globals()[name]=[]\n",
    "                    for missing in missings:\n",
    "                        variable=f'{synthesizer}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])\n",
    "            else:\n",
    "                for row in rows:\n",
    "                    name=f\"{synthesizer}_{row}_{dependence}\"\n",
    "                    globals()[name]=[]\n",
    "                    for missing in missings:\n",
    "                        variable=f'{synthesizer}_{dependence}_data_{row}_{missing}'\n",
    "                        globals()[name].append(globals()[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the lists of data frames for each synthesizer/simulated data type. All synthesizer dataframes in one list\n",
    "def mergeList(list1,list2,list3):\n",
    "    merged_list = list1+list2+list3\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age feature from date of birth\n",
    "def age(df):\n",
    "    today=datetime.today()\n",
    "    df['dateOfBirth']=pd.to_datetime(df['dateOfBirth'])\n",
    "    df['age']=df['dateOfBirth'].apply(lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing dataframes for random forest\n",
    "def preprocessing(df_list):\n",
    "    categorical_cols=['countryCode','gender','pilStatus','Language']\n",
    "    for df in df_list:\n",
    "        age(df)\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        df[categorical_cols]=df[categorical_cols].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest predicting ratings. Returns MAE.\n",
    "def rf(data):\n",
    "    X = data.drop(['rating', 'id', 'hashedId', 'arcsId', 'dateOfBirth', 'emailAddress', 'generation'], axis=1, errors='ignore')\n",
    "    y = data['rating']\n",
    "\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['category','object']).columns\n",
    "    #Scaling data\n",
    "    scaler = StandardScaler()\n",
    "    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        boosting_type='rf',\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        subsample=0.8,\n",
    "        subsample_freq=1,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=5,\n",
    "        device='cpu',\n",
    "        random_state=123,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "    return -cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for getting the MAE from the random forest models for each synthetic dataset\n",
    "def randomForestLoop(simulatedList,gcList,ctganList,tvaeList):\n",
    "    row_names = [\"10000 rows - 0% Missing\", \"10000 rows - 10% Missing\", \"10000 rows - 20% Missing\",\n",
    "             \"25000 rows - 0% Missing\", \"25000 rows - 10% Missing\", \"25000 rows - 20% Missing\",\n",
    "             \"50000 rows - 0% Missing\", \"50000 rows - 10% Missing\", \"50000 rows - 20% Missing\"]\n",
    "\n",
    "    col_names = [\n",
    "        \"Simulated\", \"GC\", \"CTGAN\", \"TVAE\" \n",
    "    ]\n",
    "    data= pd.DataFrame(index=row_names, columns=col_names)\n",
    "    for i, row_name in enumerate(row_names):\n",
    "        print(row_names[i])\n",
    "        print(\"Simulated\")\n",
    "        data.at[row_names[i], \"Simulated\"] = rf(simulatedList[i])\n",
    "        print(\"GC\")\n",
    "        data.at[row_names[i], \"GC\"] = rf(gcList[i])\n",
    "        print(\"CTGAN\")\n",
    "        data.at[row_names[i], \"CTGAN\"] = rf(ctganList[i])\n",
    "        print(\"TVAE\")\n",
    "        data.at[row_names[i], \"TVAE\"] = rf(tvaeList[i])\n",
    "        \n",
    "    return data                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting path for reading in data\n",
    "script_dir = pathlib.Path().resolve()\n",
    "os.chdir(script_dir)\n",
    "parent_dir=script_dir.parent\n",
    "\n",
    "#Setting up sample sizes and missing percentages for reading in data\n",
    "rows = [10000, 25000, 50000]\n",
    "missings = [0, 10, 20]\n",
    "\n",
    "#Reading in data\n",
    "readSimulatedData(\"Simulated Data\",\"simulated_data\",parent_dir)\n",
    "readSyntheticData(\"Synthetic Data\",parent_dir)\n",
    "\n",
    "#Merging into simulated and synthetic data into lists\n",
    "simulated_lists([\"data\"],[\"10000\",\"25000\",\"50000\"],[\"0\",\"10\",\"20\"])\n",
    "synthesizer_lists([\"GC\",\"CTGAN\",\"TVAE\"],[\"data\"],[\"10000\",\"25000\",\"50000\"],[\"0\",\"10\",\"20\"])\n",
    "Simulated_data=mergeList(data_10000, data_25000, data_50000)\n",
    "GC_data=mergeList(GC_10000_data,GC_25000_data,GC_50000_data)\n",
    "CTGAN_data=mergeList(CTGAN_10000_data,CTGAN_25000_data,CTGAN_50000_data)\n",
    "TVAE_data=mergeList(TVAE_10000_data,TVAE_25000_data,TVAE_50000_data)\n",
    "\n",
    "#Preprocessing datasets for random forest\n",
    "preprocessing(Simulated_data)\n",
    "preprocessing(GC_data)\n",
    "preprocessing(CTGAN_data)\n",
    "preprocessing(TVAE_data)\n",
    "\n",
    "#Running random forest models on datasets\n",
    "dataMAE=randomForestLoop(Simulated_data,GC_data,CTGAN_data,TVAE_data)\n",
    "\n",
    "#Extracting simulated MAE for plots in other files\n",
    "simulated_MAE=dataMAE[['Simulated']]\n",
    "\n",
    "#Saving data\n",
    "dataMAE.to_csv(f\"{script_dir}/maeSingleRun1.csv\")\n",
    "simulated_MAE.to_csv(f\"{script_dir}/simulated_MAE1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
