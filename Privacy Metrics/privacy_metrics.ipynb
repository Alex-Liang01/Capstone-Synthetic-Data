{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "## For synthesis\n",
    "from sdv.single_table import GaussianCopulaSynthesizer, CTGANSynthesizer, TVAESynthesizer\n",
    "from sdv.datasets.local import load_csvs\n",
    "from sdv.metadata import Metadata\n",
    "## For evaluating\n",
    "\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from syntheval import SynthEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf62a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in different datasets\n",
    "def read_Data(dependency):\n",
    "\n",
    "    #Setting path to read data from\n",
    "    if dependency==\"simulated\":\n",
    "        prefix=\"\"\n",
    "        all_files = load_csvs(f'{parent_dir}/Simulated Data/')\n",
    "    elif dependency==\"high\":\n",
    "        prefix=\"high_\"\n",
    "        all_files = load_csvs(f'{parent_dir}/Simulating Data/Dependency/DataWithRelations/')\n",
    "    elif dependency==\"moderate\":\n",
    "        prefix=\"moderate_\"\n",
    "        all_files = load_csvs(f'{parent_dir}/Simulating Data/Dependency/DataModerateRelations/')\n",
    "    else:\n",
    "        prefix=\"low_\"\n",
    "        all_files = load_csvs(f'{parent_dir}/Simulating Data/Dependency/DataNoRelations/')\n",
    "    \n",
    "    #Reading in all datasets of different sample sizes and missing percentages\n",
    "    df_10000_0 = all_files[f'{prefix}10000_obs_0_percent_missing']\n",
    "    df_10000_10 = all_files[f'{prefix}10000_obs_10_percent_missing']\n",
    "    df_10000_20 = all_files[f'{prefix}10000_obs_20_percent_missing']\n",
    "    df_25000_0 = all_files[f'{prefix}25000_obs_0_percent_missing']\n",
    "    df_25000_10 = all_files[f'{prefix}25000_obs_10_percent_missing']\n",
    "    df_25000_20 = all_files[f'{prefix}25000_obs_20_percent_missing']\n",
    "    df_50000_0 = all_files[f'{prefix}50000_obs_0_percent_missing']\n",
    "    df_50000_10 = all_files[f'{prefix}50000_obs_10_percent_missing']\n",
    "    df_50000_20 = all_files[f'{prefix}50000_obs_20_percent_missing']    \n",
    "\n",
    "    return df_10000_0, df_10000_10, df_10000_20,df_25000_0, df_25000_10, df_25000_20,df_50000_0, df_50000_10, df_50000_20,dependency         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def syntheval_preprocess(df, syn_df, holdout_df=None):\n",
    "    #Preprocessing into form for Synth Eval \n",
    "    cols_to_drop = ['id','hashedId', 'arcsId', 'dateOfBirth']\n",
    "    df_real = df.drop(cols_to_drop, axis=1)\n",
    "    df_fake = syn_df.drop(cols_to_drop, axis=1)\n",
    "    if holdout_df is not None:\n",
    "        df_holdout = holdout_df.drop(cols_to_drop, axis=1)\n",
    "        return df_real, df_fake, df_holdout\n",
    "    return df_real, df_fake\n",
    "\n",
    "#Calculating MIA score\n",
    "def mia_score(recall,precision):\n",
    "    return 2*(recall*precision)/(recall+precision)\n",
    "\n",
    "# Synthesize and privacy protection metrics\n",
    "def synthesize_data(train_df, n_rows, holdout_df=None, seed=42):\n",
    "\n",
    "    #Creating metadata for synthesizers\n",
    "    metadata = Metadata.detect_from_dataframe(data=train_df)\n",
    "\n",
    "    metadata.update_column(column_name=\"arcsId\", sdtype=\"id\")\n",
    "    metadata.update_column(column_name=\"hashedId\", sdtype=\"id\")\n",
    "    metadata.update_column(column_name=\"countryCode\",sdtype=\"categorical\")\n",
    "    metadata.update_column(column_name=\"Language\",sdtype=\"categorical\")\n",
    "\n",
    "    # Train synthesizers\n",
    "    synthesizers = {\n",
    "        'GC': GaussianCopulaSynthesizer(metadata),\n",
    "        'CTGAN': CTGANSynthesizer(metadata,cuda=True),\n",
    "        'TVAE': TVAESynthesizer(metadata,cuda=True)\n",
    "    }\n",
    "\n",
    "    print('Training Synthetic Data')\n",
    "\n",
    "    # Generate synthetic data \n",
    "    synthetic_data = {}\n",
    "    for name, syn in synthesizers.items():\n",
    "        print(f'Training {name}')\n",
    "        syn.fit(train_df)\n",
    "        print(f'Finished Training {name}')\n",
    "        synthetic_data[name] = syn.sample(num_rows=n_rows)\n",
    "        synthetic_data[name]['dateOfBirth']=synthetic_data[name]['dateOfBirth'].astype(str)\n",
    "\n",
    "    # Privacy evaluation\n",
    "    results = {}\n",
    "    sens = ['id', 'dateOfBirth', 'emailAddress']\n",
    "    train_df['dateOfBirth']=train_df['dateOfBirth'].astype(str)\n",
    "    \n",
    "    for name, syn_df in synthetic_data.items():\n",
    "        try:\n",
    "            # NNDR, NNAA, and MIA via Syntheval\n",
    "            print(f'NNDR and NNAA {name}')\n",
    "            real, fake, holdout = syntheval_preprocess(train_df, syn_df, holdout_df)\n",
    "            evaluator = SynthEval(real,holdout_dataframe=holdout)\n",
    "            privacy_metrics = evaluator.evaluate(fake, sens, presets_file='privacyMetrics.json')\n",
    "\n",
    "            print(f'Finished NNDR and NNAA {name}')\n",
    "\n",
    "            mia=mia_score(privacy_metrics.iloc[4]['val'],privacy_metrics.iloc[5]['val'])\n",
    "\n",
    "            results[name] = {\n",
    "                'nndr': privacy_metrics.iloc[0]['val'],\n",
    "                'nnaa': privacy_metrics.iloc[2]['val'],\n",
    "                 'mia': mia\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving privacy metrics\n",
    "def save_Metrics(dependency,df_results):\n",
    "    if dependency==\"simulated\":\n",
    "        path=''\n",
    "    elif dependency==\"high\":\n",
    "        path=f'{parent_dir}/Simulating Data/Dependency/Dependency Synthetic Data/High/Privacy Metrics/'\n",
    "    elif dependency==\"moderate\":\n",
    "        path=f'{parent_dir}/Simulating Data/Dependency/Dependency Synthetic Data/Moderate/Privacy Metrics/'\n",
    "    else:\n",
    "        path=f'{parent_dir}/Simulating Data/Dependency/Dependency Synthetic Data/Low/Privacy Metrics/'\n",
    "        \n",
    "    df_results.to_csv(f'{path}privacy_metrics1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entirePipeline(dependency):\n",
    "    #Reading data\n",
    "    df_10000_0, df_10000_10, df_10000_20,df_25000_0, df_25000_10, df_25000_20,df_50000_0, df_50000_10, df_50000_20,dependency=read_Data(dependency)\n",
    "\n",
    "    df_dict = {\n",
    "        '10000_0': df_10000_0,\n",
    "        '10000_10': df_10000_10,\n",
    "        '10000_20': df_10000_20,\n",
    "        '25000_0': df_25000_0,\n",
    "        '25000_10': df_25000_10,\n",
    "        '25000_20': df_25000_20,\n",
    "        '50000_0': df_50000_0,\n",
    "        '50000_10': df_50000_10,\n",
    "        '50000_20': df_50000_20\n",
    "    }\n",
    "\n",
    "    # Train test split\n",
    "    train_sets = {}\n",
    "    holdout_sets = {}\n",
    "    for name, df in df_dict.items():\n",
    "        # Convert dateOfBirth to string to avoid datetime issues\n",
    "        df['dateOfBirth'] = pd.to_datetime(df['dateOfBirth'], errors='coerce')\n",
    "\n",
    "        train, holdout = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_sets[name] = train\n",
    "        holdout_sets[name] = holdout\n",
    "\n",
    "    # Execute pipeline\n",
    "    all_results = {}\n",
    "    for i, (name, train_df) in enumerate(train_sets.items()):\n",
    "        print(f\"Processing {name}...\")\n",
    "        #Training and synthesizing data and privacy metrics\n",
    "        all_results[name] = synthesize_data(\n",
    "            train_df=train_df,\n",
    "            n_rows=len(train_df),\n",
    "            holdout_df=holdout_sets[name]\n",
    "        )\n",
    "        \n",
    "    # Wrangling and saving results\n",
    "    results = []\n",
    "    for dataset, methods in all_results.items():\n",
    "        for method, metrics in methods.items():\n",
    "            results.append({\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'NNDR': f\"{metrics['nndr']:.3f}\",\n",
    "                'NNAA': f\"{metrics['nnaa']:.3f}\",\n",
    "                'MIA Risk': f\"{metrics['mia']:.3f}\" if metrics['mia'] is not None else \"N/A\"\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    save_Metrics(dependency,df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78814c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = pathlib.Path().resolve()\n",
    "os.chdir(script_dir)\n",
    "parent_dir=script_dir.parent\n",
    "\n",
    "# Running entire pipeline for all association strengths\n",
    "entirePipeline(\"simulated\")\n",
    "entirePipeline(\"high\")\n",
    "entirePipeline(\"moderate\")\n",
    "entirePipeline(\"low\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
